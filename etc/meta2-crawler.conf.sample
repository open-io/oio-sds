[meta2-crawler]
namespace = OPENIO
user = openio
# Comma separated list of volumes to watch
volume_list = /var/lib/oio/sds/vol1/NS/meta2-1/,/var/lib/oio/sds/vol1/NS/meta2-2/

# Wait random time before starting
# to avoid all the crawlers from working at the same time.
wait_random_time_before_starting = False
# The crawler stores a marker in a file to be able to resume after being
# stopped/restarted. Default to False.
# use_marker = False
# In seconds, the interval between two full scans. Defaults to half an hour.
interval = 1800
# In seconds, the interval between two logs entries (for each volume)
# Defaults to 300 seconds.
report_interval = 300
# Maximum containers to be scanned per second. Defaults to 10.
scanned_per_second = 10
# Number of chunks to check before updating the markers
# (not used if <use_marker> is disabled). Default to 900.
# This value represents 60s at max rate.
# scanned_between_markers = 900

# Common log stuff
log_level = INFO
log_facility = LOG_LOCAL0
log_address = /dev/log
syslog_prefix = OIO,OPENIO,meta2-crawler,1

statsd_prefix = meta2-crawler

[pipeline:main]
pipeline = logger draining auto_vacuum auto_sharding indexer

[filter:indexer]
# Index meta2 databases into the associated rdir service(s).
use = egg:oio#indexer
# Check if the meta2 database is in peers list.
# If it is an orphan, the meta2 database is not reindexed.
check_orphan = True
# Delay in seconds before trying to delete (or move) an orphan database.
delete_delay = 86400.0
# If true, in the event where an indexing worker detects that a volume it's
# trying to index does not manage a database it stumbled  upon, the indexer
# will attempt to remove any existing index for this database from the volume's
# rdir index. Also, the meta2 database file will be moved to the "orphans"
# directory of the volume. USE AT YOUR OWN RISK.
# Inconsistencies in the proxy cache can for example help induce this effect
# even when unwarranted.
remove_orphan = False

[filter:auto_sharding]
# Trigger the sharding for given container.
use = egg:oio#auto_sharding
# Size of the meta2 database from which sharding can be triggered.
sharding_db_size = 1073741824
# Strategy to use to shard thee container.
sharding_strategy = shard-with-partition
# Parameters to use for the given strategy.
## Minimum number of objects to trigger sharding.
## Notice that this number is a safeguard against concurrent sharding requests.
## Only the database size is checked by the crawler,
## this threshold is a parameter for the sharding strategy
## and won't be checked by the crawler.
sharding_threshold = 10000
## Partition to use for cutting the container.
sharding_partition = 50,50
# Try to clean up the copy before creating the new shard
# (in order to reduce the database size before replicating it and
# in order to no longer clean up a database accessible by client requests).
sharding_preclean_new_shards = True
# Maximum amount of time the sharding process is allowed
# to preclean shard copie
sharding_preclean_timeout = 120
# Maximum waiting time on the client side.
# But on server side, the request will not lock the database for more than 1 second.
sharding_replicated_clean_timeout = 30
# Maximum amount of time the sharding process is allowed to create new shard
sharding_create_shard_timeout = 120
# Maximum amount of time the sharding process is allowed to save writes
# before applying them directly to the new shards
sharding_save_writes_timeout = 120
# Time before considering that one of the sharding steps is blocked
sharding_step_timeout = 600
# Size of the meta2 database from which shrinking can be triggered
# (except for the one and last shard).
shrinking_db_size = 268435456

[filter:auto_vacuum]
# Trigger the vacuum for given container.
use = egg:oio#auto_vacuum
# Minimum waiting time after the last modification
# to be sure the container is no longer in use,
# so as not to interfere with customer requests.
min_waiting_time_after_last_modification = 30
# Trigger the vacuum when the ratio is reached
# AND the base has not been changed recently
# (cf. "min_waiting_time_after_last_modification").
soft_max_unused_pages_ratio = 0.1
# Force the vacuum to be triggered when the ratio is reached
# (ignore "min_waiting_time_after_last_modification").
hard_max_unused_pages_ratio = 0.2
# Set the timeout of the vacuum operation.
vacuum_timeout = 30.0

[filter:logger]
# Log info for for given container.
use = egg:oio#logger

[filter:draining]
# Trigger the draining for a given container.
use = egg:oio#draining
# Drain limit for each call from the crawler to the meta2. It aims to limit
# the number of draining events generated.
drain_limit = 1000
# Drain limit for each pass/iteration of the crawler. It aims to limit the
# meta2 usage and to smooth out the calls over time.
drain_limit_per_pass = 100000
# List of endpoints to fetch Redpanda cluster metrics, comma-separated.
kafka_cluster_health_metrics_endpoints =
# Kafka metrics cache duration in seconds.
kafka_cluster_health_cache_duration = 10
# List of topic patterns to watch, comma-separated.
# Only one wildcard is allowed per topic pattern.
kafka_cluster_health_topics = *
# Maximum lag allowed by watched topic (<0 to disable).
kafka_cluster_health_max_lag = -1
# Minimal available space allowed in percent (<0 to disable).
kafka_cluster_health_min_available_space = -1

[filter:verify_chunk_placement]
# Identify misplaced chunks after meta2db scan and tag them with misplaced header.
# This filter is launched by a second instance of meta2 crawler. The second meta2
# instance is a oneshot service that we can launch manually to execute only
# verify_chunk_placement filter.
use = egg:oio#verify_chunk_placement
# Maximum object scanned per second by the filter
max_scanned_per_second  = 10000
# Time interval after which service data are updated
service_update_interval = 3600
# Before checking chunks placement, the placement checker
# verfies if all chunks are there, if not it will launch
# a rebuild on missing chunks. By setting dry_run_rebuild
# to True, it will show the number of chunks that would be
# rebuilt without actually rebuilding them.
dry_run_rebuild = False
# if True placement checker will identify the misplaced chunks
# but will not tag them as so.
dry_run_checker = False
